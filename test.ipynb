{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "l = [1,2,3,4]\n",
    "\n",
    "p = pd.DataFrame([],columns=['a','b','c'])\n",
    "\n",
    "p.iloc[len(p),:] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dict= {\n",
    "  \"Name\": \"Praveen Srikaram\",\n",
    "  \"Contact Number\": \"(+91)7013815534\",\n",
    "  \"Email ID\": \"praveensaikrishna7722@gmail.com\",\n",
    "  \"Phone Number\": \"(+91)7013815534\",\n",
    "  \"Skillsets\": [\n",
    "    \"Data analysis\",\n",
    "    \"Full stack development\",\n",
    "    \"Data cleansing and preprocessing\",\n",
    "    \"Creating dashboards, reports, and visualizations\",\n",
    "    \"Cloud domain knowledge\"\n",
    "  ],\n",
    "  \"Past Job Experience\": [\n",
    "    {\n",
    "      \"Company\": \"TATA CONSULTANCY SERVICES\",\n",
    "      \"Position\": \"Data Analyst-Systems Engineer\",\n",
    "      \"Duration\": \"JAN 2022 – Till date\",\n",
    "      \"Description\": \"Worked on data analysis activities which involved thorough cleansing, preprocessing and validation of data and producing timely business reports / dashboards\"\n",
    "    },\n",
    "    {\n",
    "      \"Company\": \"TATA CONSULTANCY SERVICES\",\n",
    "      \"Position\": \"Assistant Systems Engineer\",\n",
    "      \"Duration\": \"MAR 2021–FEB 2022\",\n",
    "      \"Description\": \"Worked on cloud cyber security feature called “Identity and Access Management” which involves usage of tools like One Identity, Oracle Unified Directory\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records(resume_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=\"We are a leading fintech company based out of Palo Alto, California. We facilitate more than a billion transactions daily. The D&A team monitors and flags fraudulent transactions. The team is looking for a data scientist who will help them sift through rich transaction data and build advanced systems to effectively mitigate fraudulent transactions.\"\n",
    "designation=\"Data scientist I\"\n",
    "min_education=\"Bachelors in engineering/ mathematics/ statistics or masters in STEM\"\n",
    "experience=\"4\"\n",
    "responsibilities=\"\"\"a. Help data ingestion from different sources like streaming data, third party data, transaction data into GCP\n",
    "b. Build executive facing dashboards by mining the data. \n",
    "c. Build fraud detection models in GCP using Kubeflow.\n",
    "d. Deploy and monitor models at scale.\n",
    "e. Present and establish findings to stakeholders.\"\"\"\n",
    "techstack=\"Python/R/C++, Google BigTable, Kubeflow, Apache Spark, Jupyter, Tensorflow, pySpark, neural networks\"\n",
    "other_tools=\"\"\n",
    "role_type=\"Permanent\"\n",
    "role_location=\"Bangalore/Hyderabad\"\n",
    "requisition_id=\"1234\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example OpenAI Python library request\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a recruiter for a technology company. \"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"You are a recruiter for a technology company. You have been asked by the team to create a job description with the below details.\n",
    "            1. About the company and role: {metadata}\n",
    "            2. Designation: {designation}\n",
    "            3. Minimum educational qualifications: {min_education}. Elaborate more in detail.\n",
    "            4. Minimum years of experience required: {experience} years\n",
    "            5. Responsibilities:{responsibilities}. Elaborate on these responsibilities as per your knowledge for the role of {designation}\n",
    "            6. Technology stack experience required: {techstack}. Add one line details for each skill/tool mentioned.\n",
    "            7. Role type: {role_type}\n",
    "            8. Role location: {role_location}\n",
    "            9. Requisition ID: {requisition_id}\n",
    "            10. Other information to include in the resume: {other_tools}\n",
    "            Please add other relevant details as you may think relevant. \"\"\",\n",
    "        },\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain, OpenAI\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":1e-10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designation=\"Data scientist I\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"\"\"Question: Let's generate a job description for {designation}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt1 = PromptTemplate(template=template1, input_variables=[\"designation\"])\n",
    "designation=\"Data scientist I\"\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = overall_chain.run(designation)\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_resume import Resume\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "engine='text-davinci-003'\n",
    "\n",
    "# pdf_path='./uploads/Anupam_Misra.pdf'\n",
    "# prompt = Resume(pdf_path)._createPrompt().replace('   ','')\n",
    "# completions = openai.Completion.create(\n",
    "#         engine=engine,\n",
    "#         prompt=prompt,\n",
    "#         max_tokens=2048,\n",
    "#         n=1,\n",
    "#         temperature=0.01,\n",
    "#     )\n",
    "# answer = completions.choices[0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data=[x.split(':') for x in answer.split(\"\\n\\n\") if x!='']\n",
    "resume_dict=dict(zip([x[0] for x in resume_data],[x[1:] for x in resume_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dict['Name'][0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = resume_dict['Name'][0].strip()\n",
    "phone = resume_dict['Contact Number'][0].strip()\n",
    "email = resume_dict['Email'][0].strip()\n",
    "skills = resume_dict['Skills'][0].strip()\n",
    "past_exp = resume_dict['Past Job Experience'][0].strip()\n",
    "education = resume_dict['Education']\n",
    "certifications = resume_dict['Certifications']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame([[1,2,3,4]],columns=['1','2','3','4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(p['1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_resume = pd.read_csv(\"parsed_resumes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_resume.loc[parsed_resume['Name']=='Anupam Misra',:] = [1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import parseResume\n",
    "anwer,data,response=parseResume('./uploads/Anupam_Misra.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_resume import Resume\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# engine='text-davinci-003'\n",
    "# pdf_path=\"./uploads/Anupam_Misra.pdf\"\n",
    "# prompt = Resume(pdf_path)._createPrompt().replace('   ','')\n",
    "# completions = openai.Completion.create(\n",
    "#         engine=engine,\n",
    "#         prompt=prompt,\n",
    "#         max_tokens=2048,\n",
    "#         n=1,\n",
    "#         temperature=0.01,\n",
    "#     )\n",
    "# answer = completions.choices[0]['text']\n",
    "# import json\n",
    "# resume_dict=json.loads(answer[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "parsed_resumes=pd.read_csv('./parsed_resumes.csv')\n",
    "list(parsed_resumes['Job_Role'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_resumes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd=\"\"\"About the Company and Role:\n",
    "Our company is a leading technology company specializing in cutting-edge solutions and innovative products. As a Data Scientist, you will join our dynamic and highly skilled team dedicated to driving data-driven insights and delivering impactful solutions. In this role, you will have the opportunity to work with cross-functional teams, collaborate on challenging projects, and contribute to the growth and success of the company.\n",
    "\n",
    "Designation: Data Scientist\n",
    "\n",
    "Minimum Educational Qualifications:\n",
    "\n",
    "Bachelor's degree in Computer Science, Statistics, Mathematics, or a related field. A strong understanding of statistical concepts and algorithms is essential.\n",
    "Minimum Years of Experience Required: 4+ years of relevant industry experience in data science, machine learning, or a related field.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Create Models: Develop and implement machine learning models and algorithms to solve complex business problems. This involves data exploration, feature engineering, model selection, and evaluation.\n",
    "Data Wrangling: Gather, clean, preprocess, and transform data from various sources. Perform data analysis and ensure data quality for effective model training and inference.\n",
    "Model Maintenance: Continuously monitor, evaluate, and refine models to improve performance and adapt to changing data patterns. Implement best practices for model versioning and deployment.\n",
    "MLOPS: Collaborate with DevOps and engineering teams to build scalable and automated machine learning pipelines. Implement tools and frameworks for model deployment, monitoring, and maintenance.\n",
    "Technology Stack Experience Required:\n",
    "\n",
    "Python: Proficiency in Python programming language for data manipulation, analysis, and modeling. Experience with Pandas and Numpy libraries for efficient data processing and numerical operations.\n",
    "Operating Systems (OS): Familiarity with various operating systems (e.g., Linux, Windows) and the command-line interface for managing and executing data science workflows.\n",
    "JSON: Knowledge of JSON (JavaScript Object Notation) for data interchange and configuration purposes.\n",
    "Role Type: Full-Time Employment (FTE)\n",
    "\n",
    "Role Location: Bengaluru, India\n",
    "\n",
    "Requisition ID: 11673393\n",
    "\n",
    "Other Information to Include in the Resume:\n",
    "\n",
    "Proficiency in Python data science libraries (e.g., Pandas, Numpy) for data manipulation and analysis.\n",
    "Strong analytical and problem-solving skills.\n",
    "Excellent communication and collaboration abilities.\n",
    "Experience in data visualization and storytelling.\n",
    "Familiarity with machine learning frameworks (e.g., TensorFlow, PyTorch) is a plus.\n",
    "Knowledge of SQL and databases is beneficial.\n",
    "Advanced degree (Master's or Ph.D.) in a related field is preferred.\"\"\"\n",
    "\n",
    "prompt = f\"\"\"You are a Specialist Recruiter. Your job is to make a python list of technical skillsets ,extracting from the job description written within the $ delimiter. Do not write anything out of context. ${jd}$ \"\"\"\n",
    "completions = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        max_tokens=2048,\n",
    "        n=1,\n",
    "        temperature=0.01,\n",
    "    )\n",
    "answer = completions.choices[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills=[x.strip('\\n') for x in answer.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_resumes=pd.read_csv('./parsed_resumes.csv')\n",
    "job_roles=list(parsed_resumes['Job_Role'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "collection  = pymongo.MongoClient(os.getenv(\"MONGO_URI\") )['Resume']['Resume']\n",
    "data=collection.find({},{'_id':0}) \n",
    "parsed_resumes = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'phone', 'email', 'skills', 'education', 'past_exp',\n",
       "       'certifications', 'job_role', 'yoe'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_resumes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_to_look_at = parsed_resumes.loc[parsed_resumes['Job_Role'] == job_roles,['Name','Skillsets','Certifications','Education','Past_Job_Experience']] # May error out need to test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_data=\"\"\"\"\"\"\n",
    "for i, v in enumerate(candidates_to_look_at.values):\n",
    "    # candidate_data+=\n",
    "    candidate_data+=str(i+1)+\". \"+v[0]+\" :: \"+\"; \".join(v[1:])+\" \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_skillsets_from_jd=\"python, mlops, google analytics, power bi, machine learning\"\n",
    "selected_roles=\"data engineer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a professional recruiter for technical companies. You will be given skills and job roles to evaluate different candidates given their names, skillsets, certifications, education and past job experience. Your job will be mention the skills which are there for each candidate and to assign scores between 1 to 100 to against each candidate. Do not assign scores to skills. The candidates with the most relevant skillsets to the job role  shall be given highest score.  The skills to check are within %. The job role is given within $ delimiter and the details of candidates are given within the # delimiter. The scores shall be printed just after the list of identified skills against each candidate. Start only with the answer. Don't justify the reason behind the scoring.\n",
    "\n",
    "%{identified_skillsets_from_jd}%\n",
    "\n",
    "$ The job role is of {\",\".join(selected_roles)} $\n",
    "\n",
    "#{candidate_data}#\n",
    "\"\"\"\n",
    "\n",
    "# completions = openai.Completion.create(\n",
    "#         engine=engine,\n",
    "#         prompt=prompt,\n",
    "#         max_tokens=2048,\n",
    "#         n=1,\n",
    "#         temperature=0,\n",
    "#     )\n",
    "# answer = completions.choices[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "model=\"gpt-3.5-turbo\",\n",
    "messages=[\n",
    "    # {\"role\": \"system\", \"content\": \"You are an experienced recruiter in a technology company having in-depth knowledge of different technical roles and their responsibilities.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt,\n",
    "    },\n",
    "],\n",
    "temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer=response.choices[0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachline=[x.split('\\n') for x in answer.split(\"\\n\\n\")]\n",
    "eachline_v2=[[y.split(\":\") for y in x] for x in eachline]\n",
    "recom=pd.DataFrame(eachline_v2, columns=[\"Name\",\"Found skills\",\"Score\"])\n",
    "recom=recom.applymap(lambda x: ''.join(x))\n",
    "recom['Found skills']=recom['Found skills'].apply(lambda x: x[7:])\n",
    "recom['Score']=recom['Score'].apply(lambda x: x[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[[y[1::2]] for y in x] for x in eachline_v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import _identify_candidates_by_job_role\n",
    "parsed_resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_resumes\n",
    "selected_roles=[\"Data scientist\",\"ML engineer\"]\n",
    "candidates_to_look_at = parsed_resumes.loc[parsed_resumes['Job_Role'].isin(selected_roles),['Skillsets','Certifications','Education','YOE']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_to_look_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_data=\"\"\"\"\"\"\n",
    "print(candidate_data)\n",
    "for i, v in enumerate(candidates_to_look_at):\n",
    "    candidate_data+=str(i+1)+\". \"\n",
    "    candidate_data+=str(v[0])\n",
    "    candidate_data+=\" :: \"\n",
    "    candidate_data+=\"; \".join(str(x) for x in v[1:])\n",
    "    candidate_data+=\" \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm import _identify_candidates_by_job_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_skillsets_from_jd=[\"python\",\"mlops\",\"machine learning\",\"azure\"]\n",
    "selected_roles=[\"ML engineer\",\"Data scientist\"]\n",
    "candidate_data = _identify_candidates_by_job_role(parsed_resumes,selected_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = f\"\"\"\n",
    "You are a professional recruiter for technical companies. You will be given skills and job roles to evaluate different candidates given their names, skillsets, certifications, education and past job experience. Your job will be mention the skills which are there for each candidate and to assign scores between 1 to 100 to against each candidate. The candidates with the most relevant skillsets to the job role  shall be given highest score.  The skills to check are within %. The job role is given within $ delimiter and the details of candidates are given within the # delimiter. The scores shall be assigned just after the list of identified skills for each candidate. Start only with the answer. Don't justify the reason behind the scoring.\n",
    "\n",
    "%{identified_skillsets_from_jd}%\n",
    "\n",
    "$ The job role is of {\",\".join(selected_roles)} $\n",
    "\n",
    "#{candidate_data}#\"\"\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "model=\"gpt-3.5-turbo\",\n",
    "messages=[\n",
    "    {\"role\": \"user\", \"content\": prompt,\n",
    "    },\n",
    "],\n",
    "temperature=0,\n",
    ")\n",
    "answer = response.choices[0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachline=[x.split('\\n') for x in answer.split(\"\\n\\n\")]\n",
    "eachline_v2=[[y.split(\":\") for y in x] for x in eachline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eachline_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(eachline_v2)\n",
    "recom=pd.DataFrame(eachline_v2, columns=[\"Found skills\",\"Score\"])\n",
    "recom=recom.applymap(lambda x: ''.join(x))\n",
    "recom['Found skills']=recom['Found skills'].apply(lambda x: x[7:])\n",
    "recom['Score']=recom['Score'].apply(lambda x: x[6:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recom[\"Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "\n",
    "## getting the mongoDB Collection: DataBase Name : Resume , Collection Name : Resume\n",
    "collection  = pymongo.MongoClient( os.getenv(\"MONGO_URI\") )['Resume']['Resume']\n",
    "df =  pd.DataFrame(collection.find())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in collection.find({\"skills\":{\"$in\":[\"Salesforce\",\"CRM\"]}}):\n",
    "#     print(i)\n",
    "\n",
    "# for index,data in collection.find({\"skills\":{\"$regex\": \"SQL\"}}):\n",
    "#     print(index,\":\",data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "  \"$or\": [\n",
    "    { \"past_exp\": { \"$regex\": \"System Analyst\"} },\n",
    "    { \"past_exp\": { \"$regex\": \"Operations Engineer\"} }\n",
    "  ]\n",
    "}\n",
    "data=collection.find(query,{'_id':0}) \n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['desired_skill'] = \"Python , Machine Learning, Cloud \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_skills = df.loc[0,'skills'].split(',')\n",
    "desired_skills = df.loc[0,'desired_skill'].split(',')\n",
    "existing_skills = [x.lower().strip() for x in existing_skills]\n",
    "desired_skills = [x.lower().strip() for x in desired_skills]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to find the intersection of two sets\n",
    "def find_intersection(row):\n",
    "    existing_skills = row['skills'].split(',')\n",
    "    desired_skills = row['desired_skill'].split(',')\n",
    "    existing_skills = [x.lower().strip() for x in existing_skills]\n",
    "    desired_skills = [x.lower().strip() for x in desired_skills]\n",
    "    return set(existing_skills).intersection(set(desired_skills))\n",
    "\n",
    "# Apply the function to populate column C\n",
    "df['matchhingg_skllls'] = df.apply(find_intersection, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skillMatch(df, desired_skills):\n",
    "    df['desired_skill'] = desired_skills\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches  = [{'id': '2', 'score': 0.252377152, 'values': []}, {'id': '0', 'score': 0.252377152, 'values': []}, {'id': '1', 'score': 0.092600964, 'values': []}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [matches[i]['score'] for i in range(len(matches))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
